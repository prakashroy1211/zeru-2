{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12575870,"sourceType":"datasetVersion","datasetId":7942229}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Load the CSV file containing the wallet addresses\ndf_wallets = pd.read_csv('/kaggle/input/zeru-ai-engineer-problem2-wallet-id/Wallet id - Sheet1.csv')  # Replace with your actual filename\nwallets = df_wallets['wallet_id'].dropna().tolist()\n\nprint(f\"Loaded {len(wallets)} wallet addresses.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T14:40:18.937235Z","iopub.execute_input":"2025-07-25T14:40:18.937486Z","iopub.status.idle":"2025-07-25T14:40:20.872466Z","shell.execute_reply.started":"2025-07-25T14:40:18.937454Z","shell.execute_reply":"2025-07-25T14:40:20.871453Z"}},"outputs":[{"name":"stdout","text":"Loaded 103 wallet addresses.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport random\nfrom tqdm import tqdm\n\n# Step 1: Load Wallets from CSV\n\nwallets_df = pd.read_csv(\"wallets.csv\")\nwallets = wallets_df[\"wallet_id\"].dropna().unique().tolist()\nprint(f\"Loaded {len(wallets)} wallets.\")\n\n# Step 2: Simulated Transaction Fetcher\n\ndef simulate_compound_activity(wallet):\n    \"\"\"\n    Mock API: Returns fake transaction metrics per wallet\n    In actual use, replace this with Covalent or DefiLlama API calls.\n    \"\"\"\n    random.seed(wallet)  # deterministic per wallet\n    return {\n        \"supply_volume\": random.uniform(0, 10000),\n        \"borrow_volume\": random.uniform(0, 8000),\n        \"repay_volume\": random.uniform(0, 9000),\n        \"liquidation_count\": random.randint(0, 5),\n        \"total_txns\": random.randint(5, 100),\n    }\n\n\n# Step 3: Feature Extraction Loop\n\nall_data = []\nfor wallet in tqdm(wallets):\n    data = simulate_compound_activity(wallet)\n    data[\"wallet_id\"] = wallet\n    all_data.append(data)\n\ndf = pd.DataFrame(all_data)\n\n# Step 4: Feature Normalization + Scoring\n\nscore_features = ['supply_volume', 'borrow_volume', 'repay_volume', 'total_txns', 'liquidation_count']\n\n# Normalize features (manual min-max scaling with fallback)\nfor col in score_features:\n    min_val = df[col].min()\n    max_val = df[col].max()\n    if max_val > min_val:\n        df[col + '_norm'] = (df[col] - min_val) / (max_val - min_val)\n    else:\n        df[col + '_norm'] = 0.0\n\n# Compute weighted score (higher liquidation = worse)\ndf['score_raw'] = (\n    0.25 * df['supply_volume_norm'] +\n    0.25 * df['repay_volume_norm'] +\n    0.20 * df['borrow_volume_norm'] +\n    0.10 * df['total_txns_norm'] +\n    0.20 * (1 - df['liquidation_count_norm'])\n)\n\n# Scale to 0–1000\nmin_score = df['score_raw'].min()\nmax_score = df['score_raw'].max()\nif max_score > min_score:\n    df['score'] = ((df['score_raw'] - min_score) / (max_score - min_score) * 1000).astype(int)\nelse:\n    df['score'] = 200  # fallback\n\n\n# Step 5: Export CSV\n\ndf[['wallet_id', 'score']].to_csv(\"wallet_scores.csv\", index=False)\nprint(\"✅ Done! Saved to `wallet_scores.csv`\")\n\n\n# Optional Debug Print\n\nprint(df[['wallet_id', 'supply_volume', 'borrow_volume', 'repay_volume', 'liquidation_count', 'total_txns', 'score']].head())","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}